{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import click\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "# model = BertForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.45k/1.45k [00:00<00:00, 3.17MB/s]\n",
      "Downloading data: 100%|██████████| 78.3M/78.3M [00:46<00:00, 1.67MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:46<00:00, 46.79s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 336.54it/s]\n",
      "Generating clean split: 100%|██████████| 316086/316086 [00:00<00:00, 352439.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset= load_dataset('sehyun66/Finnhub-News','clean',split='clean')\n",
    "df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Dataset' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ssu36/tiger/NH_competition/sentiment_bert.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/NH_competition/sentiment_bert.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m dataset[\u001b[39m'\u001b[39;49m\u001b[39mnew_cloumns\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mmap\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Dataset' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "dataset['new_cloumns'] = dataset.map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=list(df['headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_inputs = tokenizer(texts, padding=True ,truncation=True,return_tensors=\"pt\")\n",
    "summary_inputs = tokenizer(texts, padding=True ,truncation=True,return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof(headline_inputs)\n",
    "sys.getsizeof(summary_inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.to_json(f\"my-dataset-{}.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= [1,1,1,1]\n",
    "sys.getsizeof(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "print(model.config.id2label[predicted_class_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(texts):\n",
    "    chunk_size= 1024\n",
    "    results=[]\n",
    "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
    "        chunk = texts[i:i+chunk_size]\n",
    "        output = tokenizer(texts, padding=True ,truncation=True,return_tensors=\"pt\")\n",
    "        results.extend(output['input_ids'])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'datetime': Value(dtype='int64', id=None),\n",
       " 'image': Value(dtype='string', id=None),\n",
       " 'related': Value(dtype='string', id=None),\n",
       " 'source': Value(dtype='string', id=None),\n",
       " 'summary': Value(dtype='string', id=None),\n",
       " 'url': Value(dtype='string', id=None),\n",
       " 'id': Value(dtype='int64', id=None),\n",
       " 'category': Value(dtype='string', id=None),\n",
       " 'headline': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model = torch.nn.DataParallel(model)\n",
    "inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 504/504 [04:37<00:00,  1.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the chunk size\n",
    "chunk_size = 1024  # Adjust this value as needed\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Loop through the input data in chunks\n",
    "for i in tqdm(range(0, len(inputs['input_ids']), chunk_size)):\n",
    "    chunk = inputs['input_ids'][i:i+chunk_size]\n",
    "    chunk.to(device)\n",
    "    # Perform inference on the chunk\n",
    "    with torch.no_grad():\n",
    "        output = model(chunk).logits   \n",
    "    # Append the output to the results list\n",
    "    results.append(output)\n",
    "\n",
    "# Concatenate or process the results as needed\n",
    "final_output = torch.cat(results, dim=0)  # Concatenate along the appropriate dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_output, 'sentiment_data/finbert.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6055, -0.4091,  0.8146], device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = final_output.to('cpu').tolist()\n",
    "df['sentiment_name'] = df['sentiment'].apply(lambda x:{'postive':x[0],'negative':x[1],'neutral':x[2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment_name'] = df['sentiment'].apply(lambda x:{'postive':x[0],'negative':x[1],'neutral':x[2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'sentiment_data/finbert.json'\n",
    "json_data = df.to_json(orient='records')\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json_file.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = load_dataset(\n",
    "        \"json\",\n",
    "        data_files={\n",
    "            \"clean\":\"/home/ssu36/tiger/NH_competition/data/clean_data/*.json\"\n",
    "        },\n",
    "        # split ={\"train\",\"clean\"}\n",
    "    )\n",
    "train = load_dataset(\n",
    "    \"json\",\n",
    "        data_files={\n",
    "            \"train\": \"/home/ssu36/tiger/NH_competition/sentiment_data/*.json\",\n",
    "        },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "ddict = DatasetDict({\n",
    "    \"train\": train,   # split1_ds is an instance of `datasets.Dataset`\n",
    "    \"clean\": clean\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 317/317 [00:00<00:00, 411.96ba/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:45<00:00, 45.92s/it]\n",
      "Downloading metadata: 100%|██████████| 874/874 [00:00<00:00, 2.48MB/s]\n"
     ]
    }
   ],
   "source": [
    "clean.push_to_hub('sehyun66/Finnhub-News','clean')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiger",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
